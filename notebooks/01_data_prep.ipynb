{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Preparation\n",
    "\n",
    "This notebook demonstrates the data collection pipeline for stock market volatility prediction using alternative data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from src.utils.config import load_config\n",
    "from src.data.finance_yf import download_price_history\n",
    "from src.data.trends_pytrends import fetch_trends\n",
    "from src.data.twitter_snscrape import fetch_tweets_for_tickers\n",
    "from src.data.reddit_praw import fetch_reddit_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "cfg = load_config()\n",
    "print(f\"Data directory: {cfg.data_dir}\")\n",
    "print(f\"Raw data: {cfg.raw_dir}\")\n",
    "print(f\"Processed data: {cfg.processed_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Financial Data Collection (Yahoo Finance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download price data for target tickers\n",
    "tickers = ['AAPL', 'MSFT', 'NVDA', 'SPY']\n",
    "start_date = '2020-01-01'\n",
    "end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Downloading data for {tickers} from {start_date} to {end_date}\")\n",
    "prices = download_price_history(tickers, start_date, end_date)\n",
    "print(f\"Downloaded {len(prices)} rows of price data\")\n",
    "print(f\"Columns: {list(prices.columns)}\")\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "for i, ticker in enumerate(tickers):\n",
    "    ax = axes[i//2, i%2]\n",
    "    close_col = f'{ticker}_close'\n",
    "    if close_col in prices.columns:\n",
    "        prices[close_col].plot(ax=ax, title=f'{ticker} Close Price')\n",
    "        ax.set_ylabel('Price ($)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Google Trends Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Google Trends data\n",
    "print(\"Fetching Google Trends data...\")\n",
    "trends = fetch_trends(tickers, start_date, end_date)\n",
    "print(f\"Downloaded {len(trends)} rows of trends data\")\n",
    "print(f\"Columns: {list(trends.columns)}\")\n",
    "trends.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize trends data\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for ticker in tickers:\n",
    "    if ticker in trends.columns:\n",
    "        trends[ticker].plot(ax=ax, label=ticker, alpha=0.7)\n",
    "ax.set_title('Google Trends Interest Over Time')\n",
    "ax.set_ylabel('Interest Score')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Social Media Data Collection\n",
    "\n",
    "Note: Twitter and Reddit data collection may take time and require API credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter data collection (using snscrape)\n",
    "print(\"Collecting Twitter data...\")\n",
    "try:\n",
    "    tweets = fetch_tweets_for_tickers(tickers, start_date, end_date, limit_per_ticker=100)\n",
    "    print(f\"Collected {len(tweets)} tweets\")\n",
    "    if not tweets.empty:\n",
    "        print(f\"Tweet columns: {list(tweets.columns)}\")\n",
    "        tweets.head()\n",
    "except Exception as e:\n",
    "    print(f\"Twitter collection failed: {e}\")\n",
    "    tweets = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit data collection (requires credentials)\n",
    "print(\"Collecting Reddit data...\")\n",
    "try:\n",
    "    start_ts = int(datetime.strptime(start_date, '%Y-%m-%d').timestamp())\n",
    "    end_ts = int(datetime.strptime(end_date, '%Y-%m-%d').timestamp())\n",
    "    reddit_posts = fetch_reddit_posts(tickers, start_ts, end_ts, limit_per_ticker=50)\n",
    "    print(f\"Collected {len(reddit_posts)} Reddit posts\")\n",
    "    if not reddit_posts.empty:\n",
    "        print(f\"Reddit columns: {list(reddit_posts.columns)}\")\n",
    "        reddit_posts.head()\n",
    "except Exception as e:\n",
    "    print(f\"Reddit collection failed: {e}\")\n",
    "    reddit_posts = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data quality\n",
    "print(\"=== Data Quality Report ===\")\n",
    "print(f\"Price data: {len(prices)} rows, {prices.isnull().sum().sum()} missing values\")\n",
    "print(f\"Trends data: {len(trends)} rows, {trends.isnull().sum().sum()} missing values\")\n",
    "print(f\"Twitter data: {len(tweets)} rows\")\n",
    "print(f\"Reddit data: {len(reddit_posts)} rows\")\n",
    "\n",
    "# Check date ranges\n",
    "print(\"\\n=== Date Ranges ===\")\n",
    "print(f\"Prices: {prices.index.min()} to {prices.index.max()}\")\n",
    "print(f\"Trends: {trends.index.min()} to {trends.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw data\n",
    "os.makedirs(cfg.raw_dir, exist_ok=True)\n",
    "\n",
    "prices.to_csv(os.path.join(cfg.raw_dir, 'prices.csv'))\n",
    "trends.to_csv(os.path.join(cfg.raw_dir, 'trends.csv'))\n",
    "\n",
    "if not tweets.empty:\n",
    "    tweets.to_csv(os.path.join(cfg.raw_dir, 'tweets.csv'), index=False)\n",
    "if not reddit_posts.empty:\n",
    "    reddit_posts.to_csv(os.path.join(cfg.raw_dir, 'reddit_posts.csv'), index=False)\n",
    "\n",
    "print(\"Raw data saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
