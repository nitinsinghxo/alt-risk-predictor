{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "This notebook demonstrates feature engineering for volatility prediction, including technical indicators, sentiment analysis, and volatility targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from src.utils.config import load_config\n",
    "from src.features.technical import add_technical_indicators\n",
    "from src.features.volatility import realized_volatility, rolling_std, garch_volatility\n",
    "from src.features.sentiment import vader_scores, finbert_scores, aggregate_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config()\n",
    "\n",
    "# Load price data\n",
    "prices = pd.read_csv(os.path.join(cfg.raw_dir, 'prices.csv'), parse_dates=['Date']).set_index('Date')\n",
    "print(f\"Loaded {len(prices)} rows of price data\")\n",
    "print(f\"Date range: {prices.index.min()} to {prices.index.max()}\")\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add technical indicators for each ticker\n",
    "tickers = ['AAPL', 'MSFT', 'NVDA', 'SPY']\n",
    "enhanced_prices = prices.copy()\n",
    "\n",
    "for ticker in tickers:\n",
    "    close_col = f'{ticker}_close'\n",
    "    high_col = f'{ticker}_high'\n",
    "    low_col = f'{ticker}_low'\n",
    "    vol_col = f'{ticker}_volume'\n",
    "    \n",
    "    if all(col in prices.columns for col in [close_col, high_col, low_col, vol_col]):\n",
    "        ticker_data = prices[[close_col, high_col, low_col, vol_col]].dropna()\n",
    "        enhanced = add_technical_indicators(ticker_data, close_col, high_col, low_col, vol_col)\n",
    "        \n",
    "        # Add enhanced features back to main dataframe\n",
    "        for col in enhanced.columns:\n",
    "            if col not in [close_col, high_col, low_col, vol_col]:\n",
    "                enhanced_prices[col] = enhanced[col]\n",
    "\n",
    "print(f\"Enhanced dataset shape: {enhanced_prices.shape}\")\n",
    "print(f\"New technical indicator columns: {[c for c in enhanced_prices.columns if any(x in c for x in ['sma', 'ema', 'rsi', 'macd', 'bb'])]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize technical indicators for AAPL\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "ticker = 'AAPL'\n",
    "close_col = f'{ticker}_close'\n",
    "\n",
    "# Price and moving averages\n",
    "axes[0,0].plot(enhanced_prices.index, enhanced_prices[close_col], label='Close', alpha=0.7)\n",
    "axes[0,0].plot(enhanced_prices.index, enhanced_prices[f'{ticker}_sma_10'], label='SMA 10', alpha=0.7)\n",
    "axes[0,0].plot(enhanced_prices.index, enhanced_prices[f'{ticker}_sma_20'], label='SMA 20', alpha=0.7)\n",
    "axes[0,0].set_title(f'{ticker} Price and Moving Averages')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# RSI\n",
    "axes[0,1].plot(enhanced_prices.index, enhanced_prices[f'{ticker}_rsi_14'], label='RSI', color='orange')\n",
    "axes[0,1].axhline(y=70, color='r', linestyle='--', alpha=0.7, label='Overbought')\n",
    "axes[0,1].axhline(y=30, color='g', linestyle='--', alpha=0.7, label='Oversold')\n",
    "axes[0,1].set_title(f'{ticker} RSI')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# MACD\n",
    "axes[1,0].plot(enhanced_prices.index, enhanced_prices[f'{ticker}_macd'], label='MACD', alpha=0.7)\n",
    "axes[1,0].plot(enhanced_prices.index, enhanced_prices[f'{ticker}_macd_signal'], label='Signal', alpha=0.7)\n",
    "axes[1,0].set_title(f'{ticker} MACD')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Bollinger Bands\n",
    "axes[1,1].plot(enhanced_prices.index, enhanced_prices[close_col], label='Close', alpha=0.7)\n",
    "axes[1,1].plot(enhanced_prices.index, enhanced_prices[f'{ticker}_bb_hband'], label='Upper BB', alpha=0.7)\n",
    "axes[1,1].plot(enhanced_prices.index, enhanced_prices[f'{ticker}_bb_lband'], label='Lower BB', alpha=0.7)\n",
    "axes[1,1].set_title(f'{ticker} Bollinger Bands')\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Volatility Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute volatility targets for each ticker\n",
    "volatility_features = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    close_col = f'{ticker}_close'\n",
    "    if close_col in enhanced_prices.columns:\n",
    "        close_prices = enhanced_prices[close_col].dropna()\n",
    "        \n",
    "        # Realized volatility (5-day forward)\n",
    "        rv = realized_volatility(close_prices, window=5)\n",
    "        \n",
    "        # Rolling standard deviation\n",
    "        roll_std = rolling_std(close_prices, window=20)\n",
    "        \n",
    "        # GARCH volatility (may take time)\n",
    "        try:\n",
    "            garch_vol = garch_volatility(close_prices)\n",
    "        except:\n",
    "            garch_vol = pd.Series(index=close_prices.index, dtype=float)\n",
    "        \n",
    "        volatility_features[ticker] = {\n",
    "            'realized_vol': rv,\n",
    "            'rolling_std': roll_std,\n",
    "            'garch_vol': garch_vol\n",
    "        }\n",
    "\n",
    "print(f\"Computed volatility features for {len(volatility_features)} tickers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize volatility measures for AAPL\n",
    "ticker = 'AAPL'\n",
    "if ticker in volatility_features:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    rv = volatility_features[ticker]['realized_vol']\n",
    "    roll_std = volatility_features[ticker]['rolling_std']\n",
    "    garch_vol = volatility_features[ticker]['garch_vol']\n",
    "    \n",
    "    ax.plot(rv.index, rv, label='Realized Volatility (5-day)', alpha=0.7)\n",
    "    ax.plot(roll_std.index, roll_std, label='Rolling Std (20-day)', alpha=0.7)\n",
    "    if not garch_vol.empty and garch_vol.notna().any():\n",
    "        ax.plot(garch_vol.index, garch_vol, label='GARCH Volatility', alpha=0.7)\n",
    "    \n",
    "    ax.set_title(f'{ticker} Volatility Measures')\n",
    "    ax.set_ylabel('Volatility')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load social media data if available\n",
    "tweets_path = os.path.join(cfg.raw_dir, 'tweets.csv')\n",
    "reddit_path = os.path.join(cfg.raw_dir, 'reddit_posts.csv')\n",
    "\n",
    "tweets = pd.DataFrame()\n",
    "reddit_posts = pd.DataFrame()\n",
    "\n",
    "if os.path.exists(tweets_path):\n",
    "    tweets = pd.read_csv(tweets_path, parse_dates=['date'])\n",
    "    print(f\"Loaded {len(tweets)} tweets\")\n",
    "\n",
    "if os.path.exists(reddit_path):\n",
    "    reddit_posts = pd.read_csv(reddit_path, parse_dates=['created_dt'])\n",
    "    print(f\"Loaded {len(reddit_posts)} Reddit posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentiment for tweets\n",
    "if not tweets.empty:\n",
    "    print(\"Analyzing tweet sentiment...\")\n",
    "    tweet_sentiment = vader_scores(tweets['content'].fillna(''))\n",
    "    tweets_with_sentiment = pd.concat([tweets, tweet_sentiment], axis=1)\n",
    "    \n",
    "    # Daily sentiment aggregation\n",
    "    daily_tweet_sentiment = aggregate_daily(tweets_with_sentiment, 'content', 'date')\n",
    "    print(f\"Daily tweet sentiment shape: {daily_tweet_sentiment.shape}\")\n",
    "    \n",
    "    # Visualize sentiment over time\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(daily_tweet_sentiment.index, daily_tweet_sentiment['compound'], label='Compound Sentiment')\n",
    "    ax.plot(daily_tweet_sentiment.index, daily_tweet_sentiment['positive'], label='Positive', alpha=0.7)\n",
    "    ax.plot(daily_tweet_sentiment.index, daily_tweet_sentiment['negative'], label='Negative', alpha=0.7)\n",
    "    ax.set_title('Daily Tweet Sentiment')\n",
    "    ax.set_ylabel('Sentiment Score')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No tweet data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentiment for Reddit posts\n",
    "if not reddit_posts.empty:\n",
    "    print(\"Analyzing Reddit sentiment...\")\n",
    "    reddit_sentiment = vader_scores(reddit_posts['title'].fillna(''))\n",
    "    reddit_with_sentiment = pd.concat([reddit_posts, reddit_sentiment], axis=1)\n",
    "    \n",
    "    # Daily sentiment aggregation\n",
    "    daily_reddit_sentiment = aggregate_daily(reddit_with_sentiment, 'title', 'created_dt')\n",
    "    print(f\"Daily Reddit sentiment shape: {daily_reddit_sentiment.shape}\")\n",
    "    \n",
    "    # Visualize sentiment over time\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(daily_reddit_sentiment.index, daily_reddit_sentiment['compound'], label='Compound Sentiment')\n",
    "    ax.plot(daily_reddit_sentiment.index, daily_reddit_sentiment['positive'], label='Positive', alpha=0.7)\n",
    "    ax.plot(daily_reddit_sentiment.index, daily_reddit_sentiment['negative'], label='Negative', alpha=0.7)\n",
    "    ax.set_title('Daily Reddit Sentiment')\n",
    "    ax.set_ylabel('Sentiment Score')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Reddit data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all features\n",
    "print(\"=== Feature Engineering Summary ===\")\n",
    "print(f\"Original price data: {prices.shape}\")\n",
    "print(f\"Enhanced with technical indicators: {enhanced_prices.shape}\")\n",
    "print(f\"Volatility features computed for {len(volatility_features)} tickers\")\n",
    "print(f\"Tweet sentiment: {len(tweets)} tweets\")\n",
    "print(f\"Reddit sentiment: {len(reddit_posts)} posts\")\n",
    "\n",
    "# Feature categories\n",
    "technical_cols = [c for c in enhanced_prices.columns if any(x in c for x in ['sma', 'ema', 'rsi', 'macd', 'bb'])]\n",
    "print(f\"\\nTechnical indicators: {len(technical_cols)} features\")\n",
    "print(f\"Volatility targets: 3 per ticker (realized, rolling, garch)\")\n",
    "print(f\"Sentiment features: VADER scores (compound, pos, neg, neu)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
