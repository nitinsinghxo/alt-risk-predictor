{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Modeling\n",
    "\n",
    "This notebook demonstrates model training and evaluation for volatility prediction using both regression and classification approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "from src.utils.config import load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config()\n",
    "dataset_path = os.path.join(cfg.processed_dir, 'dataset.csv')\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {dataset_path}. Run build_dataset.py first.\")\n",
    "\n",
    "df = pd.read_csv(dataset_path, parse_dates=['date'])\n",
    "print(f\"Loaded dataset with shape: {df.shape}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"Tickers: {df['ticker'].unique()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and targets\n",
    "feature_cols = [c for c in df.columns if c not in {'date','ticker','rv_future','vol_bucket'} and df[c].dtype != 'O']\n",
    "X = df[feature_cols].ffill().bfill().fillna(0.0)\n",
    "y_reg = df['rv_future']\n",
    "y_clf = df['vol_bucket']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Regression target (rv_future) - missing: {y_reg.isnull().sum()}\")\n",
    "print(f\"Classification target (vol_bucket) - missing: {y_clf.isnull().sum()}\")\n",
    "\n",
    "# Feature importance preview\n",
    "print(f\"\\nSample features: {feature_cols[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Regression target distribution\n",
    "y_reg_clean = y_reg.dropna()\n",
    "axes[0].hist(y_reg_clean, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Distribution of Realized Volatility (Target)')\n",
    "axes[0].set_xlabel('Volatility')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Classification target distribution\n",
    "y_clf_clean = y_clf.dropna()\n",
    "y_clf_clean.value_counts().plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Distribution of Volatility Buckets')\n",
    "axes[1].set_xlabel('Risk Level')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Regression target stats:\")\n",
    "print(f\"  Mean: {y_reg_clean.mean():.4f}\")\n",
    "print(f\"  Std: {y_reg_clean.std():.4f}\")\n",
    "print(f\"  Min: {y_reg_clean.min():.4f}\")\n",
    "print(f\"  Max: {y_reg_clean.max():.4f}\")\n",
    "print(f\"\\nClassification target distribution:\")\n",
    "print(y_clf_clean.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regression Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare regression data\n",
    "mask_reg = y_reg.notna()\n",
    "X_reg = X.loc[mask_reg]\n",
    "y_reg_clean = y_reg.loc[mask_reg]\n",
    "\n",
    "print(f\"Regression dataset: {X_reg.shape[0]} samples, {X_reg.shape[1]} features\")\n",
    "\n",
    "# Train-test split\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg_clean, test_size=0.2, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_reg.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_reg.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regression models\n",
    "reg_models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=400, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=3)\n",
    "}\n",
    "\n",
    "reg_results = {}\n",
    "\n",
    "for name, model in reg_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_reg, y_train_reg)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_reg)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test_reg, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test_reg, y_pred)\n",
    "    \n",
    "    reg_results[name] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }\n",
    "    \n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "# Save best regression model\n",
    "best_reg_name = min(reg_results.keys(), key=lambda x: reg_results[x]['rmse'])\n",
    "best_reg_model = reg_results[best_reg_name]['model']\n",
    "joblib.dump(best_reg_model, os.path.join(cfg.processed_dir, 'models', 'best_regression_model.pkl'))\n",
    "print(f\"\\nBest regression model: {best_reg_name} (RMSE: {reg_results[best_reg_name]['rmse']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regression results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "for i, (name, results) in enumerate(reg_results.items()):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Scatter plot: actual vs predicted\n",
    "    ax.scatter(y_test_reg, results['predictions'], alpha=0.6)\n",
    "    ax.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)\n",
    "    ax.set_xlabel('Actual Volatility')\n",
    "    ax.set_ylabel('Predicted Volatility')\n",
    "    ax.set_title(f'{name}\\nRMSE: {results[\"rmse\"]:.4f}, R²: {results[\"r2\"]:.4f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare classification data\n",
    "mask_clf = y_clf.notna()\n",
    "X_clf = X.loc[mask_clf]\n",
    "y_clf_clean = y_clf.loc[mask_clf]\n",
    "\n",
    "print(f\"Classification dataset: {X_clf.shape[0]} samples, {X_clf.shape[1]} features\")\n",
    "print(f\"Class distribution:\")\n",
    "print(y_clf_clean.value_counts())\n",
    "\n",
    "# Train-test split\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_clf, y_clf_clean, test_size=0.2, random_state=42, shuffle=False, stratify=y_clf_clean\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_clf.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_clf.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classification models\n",
    "clf_models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, max_depth=3)\n",
    "}\n",
    "\n",
    "clf_results = {}\n",
    "\n",
    "for name, model in clf_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_clf, y_train_clf)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_clf)\n",
    "    y_pred_proba = model.predict_proba(X_test_clf)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    f1 = f1_score(y_test_clf, y_pred, average='weighted')\n",
    "    \n",
    "    clf_results[name] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test_clf, y_pred))\n",
    "\n",
    "# Save best classification model\n",
    "best_clf_name = max(clf_results.keys(), key=lambda x: clf_results[x]['f1'])\n",
    "best_clf_model = clf_results[best_clf_name]['model']\n",
    "joblib.dump(best_clf_model, os.path.join(cfg.processed_dir, 'models', 'best_classification_model.pkl'))\n",
    "print(f\"\\nBest classification model: {best_clf_name} (F1: {clf_results[best_clf_name]['f1']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "for i, (name, results) in enumerate(clf_results.items()):\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test_clf, results['predictions'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i, 0])\n",
    "    axes[i, 0].set_title(f'{name} - Confusion Matrix')\n",
    "    axes[i, 0].set_xlabel('Predicted')\n",
    "    axes[i, 0].set_ylabel('Actual')\n",
    "    \n",
    "    # Feature importance (for Random Forest)\n",
    "    if hasattr(results['model'], 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X_clf.columns,\n",
    "            'importance': results['model'].feature_importances_\n",
    "        }).sort_values('importance', ascending=False).head(10)\n",
    "        \n",
    "        axes[i, 1].barh(range(len(feature_importance)), feature_importance['importance'])\n",
    "        axes[i, 1].set_yticks(range(len(feature_importance)))\n",
    "        axes[i, 1].set_yticklabels(feature_importance['feature'])\n",
    "        axes[i, 1].set_title(f'{name} - Top 10 Feature Importance')\n",
    "        axes[i, 1].set_xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary\n",
    "print(\"=== MODEL PERFORMANCE SUMMARY ===\")\n",
    "print(\"\\nRegression Models:\")\n",
    "for name, results in reg_results.items():\n",
    "    print(f\"  {name}: RMSE={results['rmse']:.4f}, R²={results['r2']:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Models:\")\n",
    "for name, results in clf_results.items():\n",
    "    print(f\"  {name}: F1={results['f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Models Saved:\")\n",
    "print(f\"  Regression: {best_reg_name} (RMSE: {reg_results[best_reg_name]['rmse']:.4f})\")\n",
    "print(f\"  Classification: {best_clf_name} (F1: {clf_results[best_clf_name]['f1']:.4f})\")\n",
    "\n",
    "print(f\"\\nModels saved to: {os.path.join(cfg.processed_dir, 'models')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
